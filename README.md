# image-to-sound

Modern research on neuroplasticity has indicated that in blind individuals, the brain can reroute non-visual sense information to the visual cortex, effectively enabling people to see by other means ([Here's a link to one such study](https://www.ncbi.nlm.nih.gov/pubmed/8606771)). This app draws inspiration from some of that research.

I'm interested in whether the brain can learn to "see" by listening to sounds which have been generated by an image. This is an app takes an image and maps it to sound in a way that should convey spatial information.

In the process of building the app, I also came across a real-world device (called the vOIce) which does pretty much the same thing, and [there have already been some success stories](https://www.wired.com/2017/03/book-excerpt-body-builders/) where blind people were able to distinguish objects and navigate their environment using the device. [You can find out more info about the vOIce here](https://www.seeingwithsound.com/). I'm not associated with that project, but I think it's pretty fascinating and you should check it out.

This app is more of a proof of concept that you can play around with in the browser to test how "seeing with sound" might work.

## How it works

Image are divided into pixels, and we can associate each pixel with some sound information. The height of a pixel in the images corresponds to its pitch. Pixels higher up will have a higher pitch, and pixels lower in the image have lower pitch. The brightness (or lumocity) of each pixel corresponds to how loud it sounds when the image is played.

Once each pixel has been associated with a pitch and a volume, the image is scanned column by column from left to right. You'll hear objects on the left side of the image first, and images on the right side after. The sound also pans from left to right as the image is scanned, to help convey spatial relationships.

## Limitations

Right now, the app can only process images up to 128 x 64 in resolution. In principle, the image processing algorithm could process larger images, but the bottleneck is actually the Web Audio API, which I have limited control over. At higher resolutions, the Web Audio API starts to run into performance problems when it tries to play back the sound.

Additionally, there are physical limits to how well humans can distinguish between pitches that are close together. At much higher resolutions, the user wouldn't be able to glean much more information from the sound anyway.

## License

This app is licensed under the [MIT License](https://github.com/amgarrett09/Audible-Sights/blob/master/LICENSE.md)